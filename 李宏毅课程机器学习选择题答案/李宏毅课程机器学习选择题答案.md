# 机器学习选择题解析加整理

三岁出品必是精品！  
整理内容源于李宏毅老师机器学习课程群提问答疑解析内容！  

## 单选题(一)
<font size=3>
  
  ***   
  > 机器学习训练时，Mini-Batch 的大小优选为2个的幂，如 256 或 512。它背后的原因是什么？  
A. Mini-Batch 为偶数的时候，梯度下降算法训练的更快  
B. Mini-Batch 设为2的幂，是为了符合CPU、GPU的内存要求，利于并行化处理  
C. 不使用偶数时，损失函数是不稳定的  
D. 以上说法都不对  

  
  答案：B  
  解析：略  
  ***  
  >下列说法错误的是？  
A. 当目标函数是凸函数时，梯度下降算法的解一般就是全局最优解  
B. 进行 PCA 降维时，需要计算协方差矩阵  
C. 沿负梯度的方向一定是最优的方向  
D. 利用拉格朗日函数能解带约束的优化问题   

  答案：C  
  解析：沿负梯度的方向是函数值减少最快的方向但不一定就是最优方向。  
***  
  > 关于 L1、L2 正则化下列说法正确的是？   
A. L2 正则化能防止过拟合，提升模型的泛化能力，但 L1 做不到这点    
B. L2 正则化技术又称为 Lasso Regularization    
C. L1 正则化得到的解更加稀疏    
D. L2 正则化得到的解更加稀疏    

  答案：C    
  解析：L1、L2 正则化都能防止过拟合，提升模型的泛化能力。L1 正则化技术又称为 Lasso Regularization。L1 正则化得到的解更加稀疏。      
***  
  
  >“增加卷积核的尺寸，一定能提高卷积神经网络的性能。” 这句话是否正确？  
A. 正确  
B. 错误  

  答案：B    
  解析：卷积核的尺寸是超参数，不一定增加其尺寸就一定增加神经网络的性能，需要验证选择最佳尺寸。  
*** 
  >有 N 个样本，一般用于训练，一般用于测试。若增大 N 值，则训练误差和测试误差之间的差距会如何变化？  
A. 增大  
B. 减小  

  答案：B    
  解析：增加数据，能够有效减小过拟合，减小训练样本误差和测试样本误差之间的差距。  
*** 
  >在回归模型中，下列哪一项在权衡欠拟合（under-fitting）和过拟合（over-fitting）中影响最大？ 
A. 多项式阶数  
B. 更新权重 w 时，使用的是矩阵求逆还是梯度下降  
C. 使用常数项  

答案：A      
  解析：选择合适的多项式阶数非常重要。如果阶数过大，模型就会更加复杂，容易发生过拟合；如果阶数较小，模型就会过于简单，容易发生欠拟合。  
***   
  >如果我们说“线性回归”模型完美地拟合了训练样本（训练样本误差为零），则下面哪个说法是正确的？  
A.测试样本误差始终为零  
B.测试样本误差不可能为零  
C.以上答案都不对  
  
  答案：C  
  解析：略

## 多选题（一）
<font size=3>
  
  >下列方法中，可以用于特征降维的方法包括？  
A. 主成分分析 PCA   
B. 线性判别分析 LDA   
C. AutoEncoder   
D. 矩阵奇异值分解 SVD   
E. 最小二乘法 LeastSquares   

  答案：ABCD  
解析：主成分分析 PCA 、线性判别分析 LDA 、AutoEncoder、矩阵奇异值分解 SVD 都是用于特征降维的方法。最小二乘法是解决线性回归问题的算法，但是并没有进行降维。  
  ***  
  >下列关于极大似然估计（Maximum Likelihood Estimate，MLE），说法正确的是？   
A. MLE 可能并不存在  
B. MLE 总是存在  
C. 如果 MLE 存在，那么它的解可能不是唯一的  
D. 如果 MLE 存在，那么它的解一定是唯一的  
  答案：AC  
解析：如果极大似然函数 L(θ) 在极大值处不连续，一阶导数不存在，则 MLE 不存在；另一种情况是 MLE 并不唯一，极大值对应两个θ。   
  ***  
>下列哪种方法可以用来减小过拟合？  
A. 更多的训练数据  
B. L1 正则化  
C. L2 正则化  
D. 减小模型的复杂度  
  
  答案：ABCD    
解析：略     





## 单选题(二)
<font size=3>
  
  ***   
  > 关于循环神经网络（RNN）描述正确的是  
A.可以用于处理序列数据  
B.不能处理可变长序列数据  
C.不同于卷积神经网络，RNN的参数不能共享  
D.隐藏层上面的unit彼此没有关联  

  答案：A  
  解析：RNN可以设置单独的句子长度参数，也能参数共享，隐藏层的神经元也是彼此作用的  
  ***  
  >下面梯度下降说法错误的是？  
A.随机梯度下降是梯度下降中常用的一种  
B.梯度下降包括随机梯度下降和批量梯度下降  
C.梯度下降算法速度快且可靠  
D.随机梯度下降是深度学习算法当中常用的优化算法之一  

  答案：C   
  解析：梯度下降一般只全量更新，效率低，所以随机梯度相比梯度下降，每次只选择部分样本做更新，效率更高，速度更快  
  ***
  >下面关于无监督学习描述正确的是  
A.无监督算法只处理“特征”，不处理“标签”  
B.降维算法不属于无监督学习  
C.K-meas算法和SVM算法都属于无监督学习  
D.以上都不对  
  
   答案：A    
  解析：SVM属于监督学习算法，降维是非监督  
  ***
  >"在一个神经网络里，知道每一个神经元的权重和偏差是最重要的一步，如果以某种方法知道了神经网络准确的权重和偏差，你就可以近似任何函数，实现这个最佳的方法是什么？  
A.随机赋值，祈祷它们是正确的  
B.搜索所有权重的偏差的组合，直到得到最佳值  
C.赋予一个初始值，通过检查跟最佳值的差值，然后迭代更新权重  
D.以上都不是  
  
  答案：C     
  解析：这是神经网络算法的原理  
  ***
>关于神经网络与深度学习的关系表述不正确的是？  
A.深度学习的概念源于人工神经网络的研究  
B.含有多个隐层的神经网络算法就是一种深度学习算法  
C.单层神经网络也是深度学习的一种  
D.卷积神经网络属于深度学习的一种  
  
  答案：C     
  解析：深度学习一般至少包含输入层，隐藏层，输出层，不是单层。  
  ***
  >以下关于卷积神经网络，说法正确的是？  
A.卷积神经网络只能有一个卷积核  
B.卷积神经网络可以有多个卷积核，但是必须同大小  
C.卷积神经网络可以有多个卷积核，可以不同大小   
D.卷积神经网络不能使用在文本这种序列数据中  

  
  答案：C       
  解析：可以多个卷积核；大小也可以在不同的层，设置不同的值；在NLP领域也可以使用CNN做特征提取  
  ***
  >以下关于逻辑回归的说法不正确的是？  
A.逻辑回归必须对缺失值做预处理  
B.逻辑回归要求自变量和目标变量是线性关系  
C.逻辑回归比决策树，更容易过度拟合  
D.逻辑回归只能做2值分类，不能直接做多值分类    

  答案：C       
  解析：决策树是更容易过拟合的   
 

## 多选题（二）
<font size=3>
  


>训练误差会降低模型的准确率，产生欠拟合，此时如何提升模拟拟合度？  
A.增加数据量  
B.特征工程    
C.减少正则化参数  
D.提高模型复杂度 
  
  答案：B、C、D         
  解析：训练误差来自模型算法本身，和数据量大小无关  
  ***
  >对于PCA说法正确的是？  
A.我们必须在使用PCA前规范化数据  
B.我们应该选择使得模型有最大variance的主成分  
C.我们应该选择使得模型有最小variance的主成分  
D.我们可以使用PCA在低维度上做数据可视化  

  答案：A、B、D       
  解析：主成分是能对数据产生巨大影响的，因此产生巨大影响的那么对于方差的影响肯定也很大，所以C不对  
  ***
  >有关集成学习下列说法正确的是？  
A.基本模型应尽量来自于同一算法，通过改变训练数据和参数，得到不同的基本模型  
B.通常来讲，基本模型之间相关性应该低一些  
C.集成的基本模型的数量越多，集成模型的效果就越好  
D.bagging boosting 时常用的集成学习的方法  
  
  答案：B、D       
  解析：集成算法可以集成不同算法模型，这也正式集成算法有效的原因；基本模型越多，也可能导致过拟合，在模型选择上，应保证效果好的情况下再考虑集成  



### 作者简介
> 作者：三岁  
经历：自学python，现在混迹于paddle社区，希望和大家一起从基础走起，一起学习Paddle  
csdn地址：https://blog.csdn.net/weixin_45623093/article/list/3  
我在AI Studio上获得至尊等级，点亮9个徽章，来互关呀~ https://aistudio.baidu.com/aistudio/personalcenter/thirdview/284366   

>传说中的飞桨社区最菜代码人，让我们一起努力！  
记住：三岁出品必是精品 （~~不要脸系列~~）
